import streamlit as st
import pandas as pd
import numpy as np
import os
from io import BytesIO, StringIO
import plotly.graph_objects as go
from plotly.io import from_json, to_json
from datetime import datetime
import pickle


# åˆå§‹åŒ–sessionçŠ¶æ€
essential_states = {
    'flow_df': None,
    'brand_mapping': None,
    'original_df': None,
    'marked_data': None,
    'split_flow_data': None,
    'sorted_split_data': None,
    'split_flow_data_start': None,
    'sorted_split_data_start': None,
    'sankey_fig_start': None,
    'split_flow_data_end': None,
    'sorted_split_data_end': None,
    'sankey_fig_end': None,
    'selected_product': None,
    'sankey_fig': None,
    'top_products': None,
    'history_snapshots': [],
    'current_snapshot_id': None,
    'uploaded_file_processed': False,
    'start_period': None,
    'end_period': None
}

for key, value in essential_states.items():
    if key not in st.session_state:
        st.session_state[key] = value

# å†å²è®°å½•å¿«ç…§é…ç½®
SNAPSHOT_FILE = "analysis_snapshots.pkl"

# å®šä¹‰ä¸­é—´å±‚æ ‡ç­¾é…ç½®
MIDDLE_LAYER_CONFIG = [
    {"label": "æœŸåˆ_æ–°å¢", "color": "rgba(153, 102, 255, 0.8)"},
    {"label": "æœŸåˆ_ä¸åŒå“ç‰Œä¸åŒäº§å“", "color": "rgba(54, 162, 235, 0.8)"},
    {"label": "æœŸåˆ_åŒå“ç‰Œä¸åŒäº§å“", "color": "rgba(153, 153, 255, 0.8)"},
    {"label": "åŒå“ç‰ŒåŒäº§å“", "color": "rgba(102, 204, 102, 0.8)"},
    {"label": "æœŸæœ«_åŒå“ç‰Œä¸åŒäº§å“", "color": "rgba(102, 255, 255, 0.8)"},
    {"label": "æœŸæœ«_ä¸åŒå“ç‰Œä¸åŒäº§å“", "color": "rgba(75, 192, 192, 0.8)"},
    {"label": "æœŸæœ«_æµå¤±", "color": "rgba(255, 99, 132, 0.8)"}
]

MIDDLE_LAYER_ORDER = [item["label"] for item in MIDDLE_LAYER_CONFIG]
LABEL_COLOR_MAP = {item["label"]: item["color"] for item in MIDDLE_LAYER_CONFIG}
LABEL_ORDER = MIDDLE_LAYER_ORDER
label_sort_mapping = {label: idx for idx, label in enumerate(LABEL_ORDER)}

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="äº§å“å“ç‰Œæµé‡åˆ†æå·¥å…·",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Tokenæ ¡éªŒå‡½æ•°ï¼ˆå®é™…ä½¿ç”¨æ—¶å¯å¯ç”¨ï¼‰
def check_token():
    token = st.sidebar.text_input("è¯·è¾“å…¥è®¿é—®ä»¤ç‰Œ", type="password")
    if token != VALID_TOKEN:
        st.error("ä»¤ç‰Œæ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥")
        st.stop()
    st.sidebar.success("ä»¤ç‰ŒéªŒè¯é€šè¿‡")

# åŠ è½½å†å²å¿«ç…§
def load_snapshots_from_file():
    try:
        if os.path.exists(SNAPSHOT_FILE):
            with open(SNAPSHOT_FILE, 'rb') as f:
                return pickle.load(f)
        return []
    except Exception as e:
        st.error(f"åŠ è½½å¿«ç…§æ–‡ä»¶å¤±è´¥: {str(e)}")
        return []

# ä¿å­˜å†å²å¿«ç…§
def save_snapshots_to_file(snapshots):
    try:
        with open(SNAPSHOT_FILE, 'wb') as f:
            pickle.dump(snapshots, f)
        return True
    except Exception as e:
        st.error(f"ä¿å­˜å¿«ç…§å¤±è´¥: {str(e)}")
        return False

# åˆå§‹åŒ–æ—¶åŠ è½½å†å²å¿«ç…§
if not st.session_state.history_snapshots:
    loaded_snapshots = load_snapshots_from_file()
    st.session_state.history_snapshots = loaded_snapshots

# ä¿å­˜å¿«ç…§
def save_snapshot(start_period, end_period, product):
    if st.session_state.sorted_split_data is None or st.session_state.sankey_fig is None:
        st.warning("æ²¡æœ‰å¯ä¿å­˜çš„åˆ†æç»“æœ")
        return False
        
    snapshot_id = datetime.now().strftime("%Y%m%d%H%M%S%f")
    try:
        snapshot = {
            "id": snapshot_id,
            "timestamp": datetime.now(),
            "metadata": {
                "start_period": start_period,
                "end_period": end_period,
                "product": product
            },
            "data": {
                "split_flow_csv": st.session_state.sorted_split_data.to_csv(index=False),
                "sankey_fig_json": to_json(st.session_state.sankey_fig),
                "split_flow_start_csv": st.session_state.sorted_split_data_start.to_csv(index=False) if st.session_state.sorted_split_data_start is not None else None,
                "sankey_fig_start_json": to_json(st.session_state.sankey_fig_start) if st.session_state.sankey_fig_start is not None else None,
                "split_flow_end_csv": st.session_state.sorted_split_data_end.to_csv(index=False) if st.session_state.sorted_split_data_end is not None else None,
                "sankey_fig_end_json": to_json(st.session_state.sankey_fig_end) if st.session_state.sankey_fig_end is not None else None,
                "marked_data_csv": st.session_state.marked_data.to_csv(index=False) if st.session_state.marked_data is not None else None
            }
        }
        
        st.session_state.history_snapshots.append(snapshot)
        st.session_state.current_snapshot_id = snapshot_id
        save_snapshots_to_file(st.session_state.history_snapshots)
        return True
    except Exception as e:
        st.error(f"åˆ›å»ºå¿«ç…§æ—¶å‡ºé”™: {str(e)}")
        return False

# ä»å¿«ç…§åŠ è½½æ•°æ®
def load_from_snapshot(snapshot_id):
    try:
        for snapshot in st.session_state.history_snapshots:
            if snapshot["id"] == snapshot_id:
                required_keys = ["split_flow_csv", "sankey_fig_json", 
                                "split_flow_start_csv", "sankey_fig_start_json",
                                "split_flow_end_csv", "sankey_fig_end_json",
                                "marked_data_csv"]
                if not all(key in snapshot["data"] for key in required_keys):
                    st.error("å¿«ç…§æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•åŠ è½½")
                    return False

                # åŠ è½½æ ¸å¿ƒæ•°æ®
                st.session_state.sorted_split_data = pd.read_csv(
                    StringIO(snapshot["data"]["split_flow_csv"])
                )
                
                st.session_state.sankey_fig = from_json(
                    snapshot["data"]["sankey_fig_json"]
                )
                
                # åŠ è½½æ–°å¢è§†å›¾æ•°æ®
                if snapshot["data"]["split_flow_start_csv"]:
                    st.session_state.sorted_split_data_start = pd.read_csv(
                        StringIO(snapshot["data"]["split_flow_start_csv"])
                    )
                    st.session_state.sankey_fig_start = from_json(
                        snapshot["data"]["sankey_fig_start_json"]
                    )
                
                if snapshot["data"]["split_flow_end_csv"]:
                    st.session_state.sorted_split_data_end = pd.read_csv(
                        StringIO(snapshot["data"]["split_flow_end_csv"])
                    )
                    st.session_state.sankey_fig_end = from_json(
                        snapshot["data"]["sankey_fig_end_json"]
                    )
                
                # åŠ è½½æ ‡è®°æ•°æ®
                if snapshot["data"]["marked_data_csv"]:
                    st.session_state.marked_data = pd.read_csv(
                        StringIO(snapshot["data"]["marked_data_csv"])
                    )
                
                # åŠ è½½å…ƒæ•°æ®
                st.session_state.selected_product = snapshot["metadata"]["product"]
                st.session_state.start_period = snapshot["metadata"]["start_period"]
                st.session_state.end_period = snapshot["metadata"]["end_period"]
                st.session_state.current_snapshot_id = snapshot_id
                
                return True
        
        st.error(f"æœªæ‰¾åˆ°IDä¸º {snapshot_id} çš„å¿«ç…§")
        return False
    except Exception as e:
        st.error(f"åŠ è½½å¿«ç…§æ—¶å‡ºé”™: {str(e)}")
        return False

# åˆ é™¤å¿«ç…§
def delete_snapshot(snapshot_id):
    try:
        st.session_state.history_snapshots = [
            s for s in st.session_state.history_snapshots 
            if s["id"] != snapshot_id
        ]
        if st.session_state.current_snapshot_id == snapshot_id:
            st.session_state.current_snapshot_id = None
        save_snapshots_to_file(st.session_state.history_snapshots)
        return True
    except Exception as e:
        st.error(f"åˆ é™¤å¿«ç…§æ—¶å‡ºé”™: {str(e)}")
        return False

# ç”Ÿæˆæ¡‘åŸºå›¾å‡½æ•°ï¼ˆä¼˜åŒ–æ ‡ç­¾é‡å½±é—®é¢˜ï¼‰
def generate_sorted_sankey(split_flow_data, top_products=None, use_full_products=False):
    def aggregate_node(node, top_products, use_full_products):
        if use_full_products or not top_products:
            return node
            
        special_tags = ["æ–°å¢é—¨åº—", "é—¨åº—æµå¤±", "äº§å“æµå¤±", "æ–°å¢äº§å“", "å…¶ä»–äº§å“"]
        if node in special_tags or node in MIDDLE_LAYER_ORDER:
            return node
            
        if node.startswith("æœŸåˆ_"):
            base_node = node[3:]
            if base_node not in top_products and base_node not in special_tags:
                return "æœŸåˆ_å…¶ä»–äº§å“"
            return node
            
        if node.startswith("æœŸæœ«_"):
            base_node = node[3:]
            if base_node not in top_products and base_node not in special_tags:
                return "æœŸæœ«_å…¶ä»–äº§å“"
            return node
            
        return node
    
    def process_source_node(node):
        if "æœŸåˆ" in node or "æ–°å¢" in node or node in MIDDLE_LAYER_ORDER:
            return node
        return f"æœŸåˆ_{node}"
    
    def process_target_node(node):
        if "æœŸæœ«" in node or "æµå¤±" in node or node in MIDDLE_LAYER_ORDER:
            return node
        return f"æœŸæœ«_{node}"
    
    processed_data = split_flow_data.copy()
    processed_data['æºèŠ‚ç‚¹'] = processed_data['æºèŠ‚ç‚¹'].apply(process_source_node)
    processed_data['ç›®æ ‡èŠ‚ç‚¹'] = processed_data['ç›®æ ‡èŠ‚ç‚¹'].apply(process_target_node)
    
    if not use_full_products and top_products is not None:
        processed_data['æºèŠ‚ç‚¹'] = processed_data['æºèŠ‚ç‚¹'].apply(
            lambda x: aggregate_node(x, top_products, use_full_products)
        )
        processed_data['ç›®æ ‡èŠ‚ç‚¹'] = processed_data['ç›®æ ‡èŠ‚ç‚¹'].apply(
            lambda x: aggregate_node(x, top_products, use_full_products)
        )
        processed_data = processed_data.groupby(
            ['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹', 'æµå‘ç±»å‹', 'æ ‡ç­¾ç±»åˆ«'], 
            as_index=False
        )['æµé‡'].sum()
    
    # æå–å„å±‚èŠ‚ç‚¹ï¼ˆå»é‡å¤„ç†ï¼‰
    layer1_nodes = []
    for _, row in processed_data.iterrows():
        if row['æµå‘ç±»å‹'] == 'æœŸåˆåˆ°æ ‡ç­¾':
            node = row['æºèŠ‚ç‚¹']
            if node not in layer1_nodes and (node.startswith("æœŸåˆ_") or node in ["æ–°å¢é—¨åº—", "æ–°å¢äº§å“"]):
                layer1_nodes.append(node)
    
    layer2_nodes = [node for node in MIDDLE_LAYER_ORDER if node in processed_data['æºèŠ‚ç‚¹'].values or node in processed_data['ç›®æ ‡èŠ‚ç‚¹'].values]
    
    layer3_nodes = []
    for _, row in processed_data.iterrows():
        if row['æµå‘ç±»å‹'] == 'æ ‡ç­¾åˆ°æœŸæœ«':
            node = row['ç›®æ ‡èŠ‚ç‚¹']
            if node not in layer3_nodes and (node.startswith("æœŸæœ«_") or node in ["é—¨åº—æµå¤±", "äº§å“æµå¤±"]):
                layer3_nodes.append(node)
    
    # å…³é”®ä¿®å¤1ï¼šèŠ‚ç‚¹å»é‡ï¼ˆé¿å…é‡å¤æ ‡ç­¾ï¼‰
    all_nodes = layer1_nodes + layer2_nodes + layer3_nodes
    unique_nodes = list(dict.fromkeys(all_nodes))  # å»é‡å¹¶ä¿ç•™é¡ºåº
    
    # è®¡ç®—ä¸­é—´å±‚ç™¾åˆ†æ¯”
    middle_layer_inflows = {}
    total_middle_inflow = 0
    for node in layer2_nodes:
        inflow = processed_data[
            (processed_data['ç›®æ ‡èŠ‚ç‚¹'] == node) & 
            (processed_data['æµå‘ç±»å‹'] == 'æœŸåˆåˆ°æ ‡ç­¾')
        ]['æµé‡'].sum()
        middle_layer_inflows[node] = inflow
        total_middle_inflow += inflow
    
    layer2_labels_with_percent = []
    for node in layer2_nodes:
        # ä¼˜åŒ–æ ‡ç­¾é•¿åº¦ï¼Œé¿å…è¿‡é•¿å¯¼è‡´é‡å 
        short_label = node.replace("ä¸åŒå“ç‰Œä¸åŒäº§å“", "è·¨å“ç‰Œ").replace("åŒå“ç‰Œä¸åŒäº§å“", "åŒå“ç‰Œè·¨äº§å“")
        if total_middle_inflow > 0:
            percentage = (middle_layer_inflows[node] / total_middle_inflow) * 100
            layer2_labels_with_percent.append(f"{short_label}\n{round(percentage)}%")
        else:
            layer2_labels_with_percent.append(short_label)
    
    # å…³é”®ä¿®å¤2ï¼šæ ‡ç­¾ä¸å»é‡èŠ‚ç‚¹ä¸€ä¸€å¯¹åº”
    labels = []
    for node in unique_nodes:
        if node in layer1_nodes:
            # ç®€åŒ–æœŸåˆèŠ‚ç‚¹æ ‡ç­¾
            simplified = node.replace("æœŸåˆ_", "")
            labels.append(simplified)
        elif node in layer2_nodes:
            idx = layer2_nodes.index(node)
            labels.append(layer2_labels_with_percent[idx])
        elif node in layer3_nodes:
            # ç®€åŒ–æœŸæœ«èŠ‚ç‚¹æ ‡ç­¾
            simplified = node.replace("æœŸæœ«_", "")
            labels.append(simplified)
    
    node_indices = {node: idx for idx, node in enumerate(unique_nodes)}
    
    # è¿æ¥é…ç½®
    links_source = [node_indices[row['æºèŠ‚ç‚¹']] for _, row in processed_data.iterrows()]
    links_target = [node_indices[row['ç›®æ ‡èŠ‚ç‚¹']] for _, row in processed_data.iterrows()]
    links_value = processed_data['æµé‡'].tolist()
    
    # é¢œè‰²é…ç½®
    def get_node_color(node):
        if node in LABEL_COLOR_MAP:
            return LABEL_COLOR_MAP[node]
        elif node.startswith("æœŸåˆ_") or node in ["æ–°å¢é—¨åº—", "æ–°å¢äº§å“"]:
            connected_labels = processed_data[processed_data['æºèŠ‚ç‚¹'] == node]['æ ‡ç­¾ç±»åˆ«'].unique()
            if len(connected_labels) > 0:
                label_color = LABEL_COLOR_MAP.get(connected_labels[0], "rgba(200, 200, 200, 0.8)")
                return label_color.replace("0.8", "0.4").replace("0.6", "0.3")
            return "rgba(200, 200, 200, 0.4)"
        elif node.startswith("æœŸæœ«_") or node in ["é—¨åº—æµå¤±", "äº§å“æµå¤±"]:
            connected_labels = processed_data[processed_data['ç›®æ ‡èŠ‚ç‚¹'] == node]['æ ‡ç­¾ç±»åˆ«'].unique()
            if len(connected_labels) > 0:
                label_color = LABEL_COLOR_MAP.get(connected_labels[0], "rgba(200, 200, 200, 0.8)")
                return label_color.replace("0.8", "0.4").replace("0.6", "0.3")
            return "rgba(200, 200, 200, 0.4)"
        else:
            return "rgba(200, 200, 200, 0.8)"
    
    node_colors = [get_node_color(node) for node in unique_nodes]
    
    link_colors = []
    for _, row in processed_data.iterrows():
        label_color = LABEL_COLOR_MAP.get(row['æ ‡ç­¾ç±»åˆ«'], "rgba(200, 200, 200, 0.8)")
        link_color = label_color.replace("0.8", "0.3").replace("0.6", "0.2")
        link_colors.append(link_color)
    
    # å…³é”®ä¿®å¤3ï¼šä¼˜åŒ–èŠ‚ç‚¹ä½ç½®è®¡ç®—ï¼ˆé¿å…é‡å ï¼‰
    node_x, node_y = [], []
    vertical_padding = 0.1  # ä¸Šä¸‹è¾¹è·
    vertical_range = 1.0 - 2 * vertical_padding  # å¯ç”¨å‚ç›´ç©ºé—´
    
    # ç¬¬ä¸€å±‚èŠ‚ç‚¹ä½ç½®ï¼ˆxå›ºå®š0.15ï¼Œå¢åŠ æ°´å¹³é—´è·ï¼‰
    level_count = len(layer1_nodes)
    for i in range(level_count):
        node_x.append(0.15)  # å¢åŠ xå€¼ï¼Œè¿œç¦»å·¦ä¾§è¾¹ç¼˜
        if level_count == 1:
            node_y.append(0.5)  # å•ä¸ªèŠ‚ç‚¹å±…ä¸­
        else:
            # æ›´å‡åŒ€çš„å‚ç›´åˆ†å¸ƒ
            node_y.append(vertical_padding + (vertical_range / (level_count - 1)) * i)
    
    # ç¬¬äºŒå±‚èŠ‚ç‚¹ä½ç½®ï¼ˆxå›ºå®š0.5ï¼Œä¸­é—´ä½ç½®ï¼‰
    level_count = len(layer2_nodes)
    for i in range(level_count):
        node_x.append(0.5)
        if level_count == 1:
            node_y.append(0.5)
        else:
            node_y.append(vertical_padding + (vertical_range / (level_count - 1)) * i)
    
    # ç¬¬ä¸‰å±‚èŠ‚ç‚¹ä½ç½®ï¼ˆxå›ºå®š0.85ï¼Œå¢åŠ æ°´å¹³é—´è·ï¼‰
    level_count = len(layer3_nodes)
    for i in range(level_count):
        node_x.append(0.85)  # å‡å°xå€¼ï¼Œè¿œç¦»å³ä¾§è¾¹ç¼˜
        if level_count == 1:
            node_y.append(0.5)
        else:
            node_y.append(vertical_padding + (vertical_range / (level_count - 1)) * i)
    
    # åˆ›å»ºæ¡‘åŸºå›¾ï¼ˆå…³é”®ä¿®å¤4ï¼šå¢å¼ºæ¸²æŸ“é…ç½®ï¼‰
    fig = go.Figure(data=[go.Sankey(
        arrangement="none",  # ç¦ç”¨è‡ªåŠ¨æ’åˆ—ï¼Œä½¿ç”¨æ‰‹åŠ¨åæ ‡
        node=dict(
            pad=20,  # å¢å¤§èŠ‚ç‚¹é—´è·ï¼Œå‡å°‘é‡å 
            thickness=25,  # å¢åŠ èŠ‚ç‚¹åšåº¦
            line=dict(color="black", width=1),  # æ›´æ¸…æ™°çš„èŠ‚ç‚¹è¾¹æ¡†
            label=labels,
            color=node_colors,
            x=node_x,
            y=node_y,
            # ä¼˜åŒ–å­—ä½“é…ç½®ï¼Œè§£å†³äº‘ç¯å¢ƒå­—ä½“é—®é¢˜
            textfont=dict(
                family="SimHei, Microsoft YaHei, Heiti TC, Arial, sans-serif",
                size=11,
                color="rgb(30, 30, 30)"
            )
        ),
        link=dict(
            source=links_source,
            target=links_target,
            value=links_value,
            color=link_colors,
            line=dict(width=0.5)  # é“¾æ¥çº¿ä¼˜åŒ–
        )
    )])
    
    # æ·»åŠ å›¾ä¾‹ï¼ˆä¼˜åŒ–å¸ƒå±€ï¼‰
    legend_items = []
    for item in MIDDLE_LAYER_CONFIG:
        try:
            rgba_str = item["color"].replace("rgba", "").replace("(", "").replace(")", "")
            r, g, b = map(lambda x: int(float(x.strip())), rgba_str.split(",")[:3])
            short_label = item["label"].replace("ä¸åŒå“ç‰Œä¸åŒäº§å“", "è·¨å“ç‰Œ").replace("åŒå“ç‰Œä¸åŒäº§å“", "åŒå“ç‰Œè·¨äº§å“")
            legend_items.append(f'<span style="color:rgb({r},{g},{b})">â—</span> {short_label}')
        except:
            legend_items.append(f'<span style="color:gray">â—</span> {item["label"]}')
    
    # å›¾ä¾‹åˆ†è¡Œæ˜¾ç¤ºï¼Œé¿å…æ°´å¹³æº¢å‡º
    legend_chunk_size = 3
    legend_rows = []
    for i in range(0, len(legend_items), legend_chunk_size):
        legend_rows.append(" ".join(legend_items[i:i+legend_chunk_size]))
    
    for i, row in enumerate(legend_rows):
        fig.add_annotation(
            text=row,
            x=0.5, y=1.05 + (i * 0.05),  # å‚ç›´æ’åˆ—å›¾ä¾‹
            xref="paper", yref="paper",
            showarrow=False, 
            font=dict(
                family="SimHei, Microsoft YaHei, Arial",
                size=10
            ),
            align="center"
        )
    
    # å›¾è¡¨å°ºå¯¸å’Œå¸ƒå±€ä¼˜åŒ–
    max_level_count = max(len(layer1_nodes), len(layer2_nodes), len(layer3_nodes))
    height = max_level_count * 60 + 250  # åŸºäºèŠ‚ç‚¹æ•°é‡åŠ¨æ€è°ƒæ•´é«˜åº¦
    height = min(max(height, 600), 1000)  # é™åˆ¶æœ€å¤§æœ€å°é«˜åº¦
    
    # å…³é”®ä¿®å¤5ï¼šå¢å¼ºå­—ä½“å…¼å®¹æ€§å’Œå¸ƒå±€ç¨³å®šæ€§
    fig.update_layout(
        title_text="äº§å“æµé‡ä¸‰å±‚æ¡‘åŸºå›¾åˆ†æ",
        font=dict(
            family="SimHei, Microsoft YaHei, Heiti TC, Arial, sans-serif",
            size=12,
            color="rgb(30, 30, 30)"
        ),
        width=1000,  # å¢åŠ å®½åº¦ï¼Œæä¾›æ›´å¤šç©ºé—´
        height=height,
        margin=dict(l=100, r=100, t=120 + (len(legend_rows)*30), b=80),  # æ ¹æ®å›¾ä¾‹è¡Œæ•°è°ƒæ•´é¡¶éƒ¨è¾¹è·
        paper_bgcolor="rgba(255, 255, 255, 1)",  # ç™½è‰²èƒŒæ™¯ï¼Œå¢å¼ºå¯¹æ¯”åº¦
        plot_bgcolor="rgba(255, 255, 255, 1)"
    )
    
    # ç¦ç”¨è‡ªåŠ¨ç¼©æ”¾ï¼Œä¿æŒå¸ƒå±€ä¸€è‡´æ€§
    fig.update_xaxes(autorange=False)
    fig.update_yaxes(autorange=False)
    
    return fig

# ç”ŸæˆæœŸæœ«åˆ†ææŠ¥å‘Š
def generate_end_report(marked_data, product):
    if marked_data is None or marked_data.empty:
        return "æ— å¯ç”¨æ•°æ®ç”ŸæˆæœŸæœ«åˆ†ææŠ¥å‘Šã€‚"
    
    relevant_data = marked_data[marked_data['æ ‡ç­¾ç±»åˆ«'] != "ä¸æ¶‰åŠç›®æ ‡äº§å“"]
    if relevant_data.empty:
        return f"æ— æ¶‰åŠäº§å“ {product} çš„ç›¸å…³æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæœŸæœ«åˆ†ææŠ¥å‘Šã€‚"
    
    start_total = relevant_data[relevant_data['æœŸåˆäº§å“'] == product]['æµé‡'].sum() / 10000
    
    if start_total == 0:
        return f"äº§å“ {product} æœŸåˆé‡‘é¢ä¸º0ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆæœŸæœ«åˆ†ææŠ¥å‘Šã€‚"
    
    same_product = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "åŒå“ç‰ŒåŒäº§å“") & 
        (relevant_data['æœŸåˆäº§å“'] == product)
    ]['æµé‡'].sum() / 10000
    
    same_brand_other = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸæœ«_åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸåˆäº§å“'] == product)
    ]['æµé‡'].sum() / 10000
    
    other_brand = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸæœ«_ä¸åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸåˆäº§å“'] == product)
    ]['æµé‡'].sum() / 10000
    
    store_loss = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸæœ«_æµå¤±") & 
        (relevant_data['æœŸåˆäº§å“'] == product) &
        (relevant_data['æœŸæœ«äº§å“'] == "é—¨åº—æµå¤±")
    ]['æµé‡'].sum() / 10000
    
    product_loss = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸæœ«_æµå¤±") & 
        (relevant_data['æœŸåˆäº§å“'] == product) &
        (relevant_data['æœŸæœ«äº§å“'] == "äº§å“æµå¤±")
    ]['æµé‡'].sum() / 10000
    
    same_brand_details = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸæœ«_åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸåˆäº§å“'] == product)
    ].groupby('æœŸæœ«äº§å“')['æµé‡'].sum().reset_index()
    same_brand_details = same_brand_details.sort_values('æµé‡', ascending=False).head(2)
    same_brand_details['æµé‡'] = same_brand_details['æµé‡'] / 10000
    
    other_brand_details = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸæœ«_ä¸åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸåˆäº§å“'] == product)
    ].groupby('æœŸæœ«äº§å“')['æµé‡'].sum().reset_index()
    other_brand_details = other_brand_details.sort_values('æµé‡', ascending=False).head(2)
    other_brand_details['æµé‡'] = other_brand_details['æµé‡'] / 10000
    
    report = f"æœŸåˆåˆ†ææŠ¥å‘Šï¼š{product}\n\n"
    report += f"äº§å“æœŸåˆé‡‘é¢ä¸º{start_total:.2f}ä¸‡ã€‚"
    report += f"æœŸæœ«ä»æ—§ä½¿ç”¨æœ¬å“çš„é‡‘é¢ä¸º{same_product:.2f}ä¸‡ï¼Œå æ¯”{same_product/start_total*100:.2f}%ï¼›"
    report += f"è½¬æ¢ä¸ºåŒå“ç‰Œçš„å…¶å®ƒäº§å“çš„é‡‘é¢ä¸º{same_brand_other:.2f}ä¸‡ï¼Œå æ¯”{same_brand_other/start_total*100:.2f}%ï¼Œ"
    
    if not same_brand_details.empty:
        details = []
        for i, row in same_brand_details.iterrows():
            details.append(f"ä¸»è¦ä¸º{row['æœŸæœ«äº§å“']}ï¼ˆé‡‘é¢{row['æµé‡']:.2f}ä¸‡ï¼Œå æ¯”{row['æµé‡']/start_total*100:.2f}%ï¼‰")
        report += "ã€".join(details) + "ï¼›"
    else:
        report += "æ— ä¸»è¦è½¬æ¢äº§å“ï¼›"
    
    report += f"è½¬æ¢ä¸ºå…¶å®ƒå“ç‰Œäº§å“çš„é‡‘é¢ä¸º{other_brand:.2f}ä¸‡ï¼Œå æ¯”{other_brand/start_total*100:.2f}%ï¼Œ"
    
    if not other_brand_details.empty:
        details = []
        for i, row in other_brand_details.iterrows():
            details.append(f"ä¸»è¦ä¸º{row['æœŸæœ«äº§å“']}ï¼ˆé‡‘é¢{row['æµé‡']:.2f}ä¸‡ï¼Œå æ¯”{row['æµé‡']/start_total*100:.2f}%ï¼‰")
        report += "ã€".join(details) + "ï¼›"
    else:
        report += "æ— ä¸»è¦è½¬æ¢äº§å“ï¼›"
    
    report += f"é—¨åº—æµå¤±é‡‘é¢ä¸º{store_loss:.2f}ä¸‡ï¼Œå æ¯”{store_loss/start_total*100:.2f}%ï¼›"
    report += f"äº§å“æµå¤±é‡‘é¢ä¸º{product_loss:.2f}ä¸‡ï¼Œå æ¯”{product_loss/start_total*100:.2f}%ã€‚\n\n"
    
    return report

# ç”ŸæˆæœŸåˆåˆ†ææŠ¥å‘Š
def generate_start_report(marked_data, product):
    if marked_data is None or marked_data.empty:
        return "æ— å¯ç”¨æ•°æ®ç”ŸæˆæœŸåˆåˆ†ææŠ¥å‘Šã€‚"
    
    relevant_data = marked_data[marked_data['æ ‡ç­¾ç±»åˆ«'] != "ä¸æ¶‰åŠç›®æ ‡äº§å“"]
    if relevant_data.empty:
        return f"æ— æ¶‰åŠäº§å“ {product} çš„ç›¸å…³æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæœŸåˆåˆ†ææŠ¥å‘Šã€‚"
    
    end_total = relevant_data[relevant_data['æœŸæœ«äº§å“'] == product]['æµé‡'].sum() / 10000
    
    if end_total == 0:
        return f"äº§å“ {product} æœŸæœ«é‡‘é¢ä¸º0ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆæœŸåˆåˆ†ææŠ¥å‘Šã€‚"
    
    same_product = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "åŒå“ç‰ŒåŒäº§å“") & 
        (relevant_data['æœŸæœ«äº§å“'] == product)
    ]['æµé‡'].sum() / 10000
    
    new_stores = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸåˆ_æ–°å¢") & 
        (relevant_data['æœŸæœ«äº§å“'] == product) &
        (relevant_data['æœŸåˆäº§å“'] == "æ–°å¢é—¨åº—")
    ]['æµé‡'].sum() / 10000
    
    new_users = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸåˆ_æ–°å¢") & 
        (relevant_data['æœŸæœ«äº§å“'] == product) &
        (relevant_data['æœŸåˆäº§å“'] == "æ–°å¢äº§å“")
    ]['æµé‡'].sum() / 10000
    
    same_brand_other = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸåˆ_åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸæœ«äº§å“'] == product)
    ]['æµé‡'].sum() / 10000
    
    other_brand = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸåˆ_ä¸åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸæœ«äº§å“'] == product)
    ]['æµé‡'].sum() / 10000
    
    same_brand_details = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸåˆ_åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸæœ«äº§å“'] == product)
    ].groupby('æœŸåˆäº§å“')['æµé‡'].sum().reset_index()
    same_brand_details = same_brand_details.sort_values('æµé‡', ascending=False).head(2)
    same_brand_details['æµé‡'] = same_brand_details['æµé‡'] / 10000
    
    other_brand_details = relevant_data[
        (relevant_data['æ ‡ç­¾ç±»åˆ«'] == "æœŸåˆ_ä¸åŒå“ç‰Œä¸åŒäº§å“") & 
        (relevant_data['æœŸæœ«äº§å“'] == product)
    ].groupby('æœŸåˆäº§å“')['æµé‡'].sum().reset_index()
    other_brand_details = other_brand_details.sort_values('æµé‡', ascending=False).head(2)
    other_brand_details['æµé‡'] = other_brand_details['æµé‡'] / 10000
    
    report = f"æœŸæœ«åˆ†ææŠ¥å‘Šï¼š{product}\n\n"
    report += f"äº§å“æœŸæœ«æ€»é‡‘é¢ä¸º{end_total:.2f}ä¸‡ã€‚"
    report += f"æœŸåˆå·²ä½¿ç”¨æœ¬å“å¹¶æŒç»­ä½¿ç”¨çš„é‡‘é¢ä¸º{same_product:.2f}ä¸‡ï¼Œå æ¯”{same_product/end_total*100:.2f}%ï¼›"
    report += f"ä»åŒå“ç‰Œå…¶ä»–äº§å“è½¬æ¢è€Œæ¥çš„æ€»é‡‘é¢ä¸º{same_brand_other:.2f}ä¸‡ï¼Œå æ¯”{same_brand_other/end_total*100:.2f}%ï¼Œ"
    
    if not same_brand_details.empty:
        details = []
        for i, row in same_brand_details.iterrows():
            details.append(f"ä¸»è¦æ¥æºä¸º{row['æœŸåˆäº§å“']}ï¼ˆé‡‘é¢{row['æµé‡']:.2f}ä¸‡ï¼Œå æ¯”{row['æµé‡']/end_total*100:.2f}%ï¼‰")
        report += "ã€".join(details) + "ï¼›"
    else:
        report += "æ— ä¸»è¦æ¥æºäº§å“ï¼›"
    
    report += f"ä»å…¶ä»–å“ç‰Œäº§å“è½¬æ¢è€Œæ¥çš„æ€»é‡‘é¢ä¸º{other_brand:.2f}ä¸‡ï¼Œå æ¯”{other_brand/end_total*100:.2f}%ï¼Œ"
    
    if not other_brand_details.empty:
        details = []
        for i, row in other_brand_details.iterrows():
            details.append(f"ä¸»è¦æ¥æºä¸º{row['æœŸåˆäº§å“']}ï¼ˆé‡‘é¢{row['æµé‡']:.2f}ä¸‡ï¼Œå æ¯”{row['æµé‡']/end_total*100:.2f}%ï¼‰")
        report += "ã€".join(details) + "ï¼›"
    else:
        report += "æ— ä¸»è¦æ¥æºäº§å“ï¼›"
    
    report += f"æ–°å¢é—¨åº—å¸¦æ¥çš„é‡‘é¢ä¸º{new_stores:.2f}ä¸‡ï¼Œå æ¯”{new_stores/end_total*100:.2f}%ï¼›"
    report += f"æ–°å¢ç”¨æˆ·å¸¦æ¥çš„é‡‘é¢ä¸º{new_users:.2f}ä¸‡ï¼Œå æ¯”{new_users/end_total*100:.2f}%ã€‚\n\n"    
    return report

# ç”Ÿæˆæ ‡ç­¾æ±‡æ€»æŠ¥å‘Š
def generate_label_summary(marked_data):
    if marked_data is None or marked_data.empty:
        return "æ— å¯ç”¨æ•°æ®ç”Ÿæˆæ ‡ç­¾æ±‡æ€»æŠ¥å‘Šã€‚"
    
    relevant_data = marked_data[marked_data['æ ‡ç­¾ç±»åˆ«'] != "ä¸æ¶‰åŠç›®æ ‡äº§å“"]
    if relevant_data.empty:
        return "æ— æœ‰æ•ˆæ•°æ®ç”Ÿæˆæ ‡ç­¾æ±‡æ€»æŠ¥å‘Šã€‚"
    
    total_flow = relevant_data['æµé‡'].sum() / 10000
    
    if total_flow == 0:
        return "æ€»æµé‡ä¸º0ï¼Œæ— æ³•ç”Ÿæˆæ ‡ç­¾æ±‡æ€»æŠ¥å‘Šã€‚"
    
    label_summary = relevant_data.groupby('æ ‡ç­¾ç±»åˆ«')['æµé‡'].sum().reset_index()
    label_summary = label_summary.sort_values(
        by='æ ‡ç­¾ç±»åˆ«', 
        key=lambda x: x.map(label_sort_mapping)
    )
    label_summary['æµé‡'] = label_summary['æµé‡'] / 10000
    label_summary['å æ¯”'] = (label_summary['æµé‡'] / total_flow) * 100
    
    gain_labels = ["æœŸåˆ_æ–°å¢", "æœŸåˆ_ä¸åŒå“ç‰Œä¸åŒäº§å“", "æœŸåˆ_åŒå“ç‰Œä¸åŒäº§å“"]
    lost_labels = ["æœŸæœ«_åŒå“ç‰Œä¸åŒäº§å“", "æœŸæœ«_ä¸åŒå“ç‰Œä¸åŒäº§å“", "æœŸæœ«_æµå¤±"]
    remain_label = "åŒå“ç‰ŒåŒäº§å“"
    
    gain_value = label_summary[label_summary['æ ‡ç­¾ç±»åˆ«'].isin(gain_labels)]['å æ¯”'].sum()
    lost_value = label_summary[label_summary['æ ‡ç­¾ç±»åˆ«'].isin(lost_labels)]['å æ¯”'].sum()
    remain_value = label_summary[label_summary['æ ‡ç­¾ç±»åˆ«'] == remain_label]['å æ¯”'].sum() if remain_label in label_summary['æ ‡ç­¾ç±»åˆ«'].values else 0
    
    lost_ratio_base_start = lost_value / (lost_value + remain_value) * 100 if (lost_value + remain_value) > 0 else 0
    gain_ratio_base_end = gain_value / (gain_value + remain_value) * 100 if (gain_value + remain_value) > 0 else 0
    start_end_comparison = (gain_value + remain_value) / (lost_value + remain_value) if (lost_value + remain_value) > 0 else 0
    
    report = "æ ‡ç­¾æµé‡æ±‡æ€»åˆ†æ\n\n"
    report += f"æ€»æµé‡ä¸º{total_flow:.2f}ä¸‡ã€‚"
    
    label_details = []
    for _, row in label_summary.iterrows():
        label = row['æ ‡ç­¾ç±»åˆ«']
        flow = row['æµé‡']
        percentage = row['å æ¯”']
        label_details.append(f"{label}çš„é‡‘é¢ä¸º{flow:.2f}ä¸‡ï¼Œå æ¯”{percentage:.2f}%")
    
    report += "ã€".join(label_details) + "ã€‚\n\n"
    
    report += "æ ¸å¿ƒæŒ‡æ ‡åˆ†æï¼š\n"
    report += f"- Gainå€¼ï¼ˆæ–°å¢ï¼ˆæ–°å¢+æµå…¥ï¼‰ï¼š{gain_value:.2f}%\n"
    report += f"- Lostå€¼ï¼ˆæµå‡º+æµå¤±ï¼‰ï¼š{lost_value:.2f}%\n"
    report += f"- Remainå€¼ï¼ˆç•™å­˜ï¼‰ï¼š{remain_value:.2f}%\n"
    report += f"- ä»¥æœŸåˆä¸ºåŸºå‡†çš„Lostå æ¯”ï¼š{lost_ratio_base_start:.2f}%\n"
    report += f"- ä»¥æœŸæœ«ä¸ºåŸºå‡†çš„Gainå æ¯”ï¼š{gain_ratio_base_end:.2f}%\n"
    report += f"- æœŸåˆæœŸæœ«å¯¹æ¯”å€¼ï¼š{start_end_comparison:.2f}å€"
    
    return report

# æ ‡é¢˜
st.title("äº§å“å“ç‰Œæµé‡åˆ†æå·¥å…·ï¼ˆæŠ¥å‘Šç‰ˆï¼‰")

# å†å²å¿«ç…§ç®¡ç†
with st.expander("å†å²åˆ†æå¿«ç…§", expanded=True):
    col1, col2 = st.columns([3, 1])
    with col1:
        st.write("å¿«ç…§åŒ…å«åˆ†æç»“æœå’Œå¯è§†åŒ–å›¾è¡¨ï¼Œå…³é—­æµè§ˆå™¨åä¸ä¼šä¸¢å¤±")
    with col2:
        if st.button("æ¸…ç©ºæ‰€æœ‰å¿«ç…§"):
            if st.session_state.history_snapshots:
                st.session_state.history_snapshots = []
                save_snapshots_to_file([])
                st.success("å·²æ¸…ç©ºæ‰€æœ‰å†å²å¿«ç…§")
                st.rerun()
            else:
                st.info("æ²¡æœ‰å¿«ç…§å¯æ¸…ç©º")
    
    # æ˜¾ç¤ºå¿«ç…§åˆ—è¡¨
    if not st.session_state.history_snapshots:
        st.info("æš‚æ— åˆ†æå¿«ç…§ï¼Œè¯·å…ˆè¿›è¡Œåˆ†æå¹¶ç”Ÿæˆç»“æœ")
    else:
        for idx, snapshot in enumerate(reversed(st.session_state.history_snapshots)):
            meta = snapshot["metadata"]
            col1, col2, col3 = st.columns([3, 1, 1])
            with col1:
                st.write(f"**{snapshot['timestamp'].strftime('%Y-%m-%d %H:%M')}** - {meta['start_period']}è‡³{meta['end_period']} - {meta['product']}")
            with col2:
                if st.button("æŸ¥çœ‹", key=f"view_{idx}_{snapshot['id']}", help=f"æŸ¥çœ‹å¿«ç…§ {snapshot['id'][-6:]}"):
                    with st.spinner(f"æ­£åœ¨åŠ è½½å¿«ç…§ {snapshot['id'][-6:]}..."):
                        load_success = load_from_snapshot(snapshot['id'])
                        if load_success:
                            st.success(f"å·²åŠ è½½å¿«ç…§ {snapshot['id'][-6:]}")
                            st.rerun()
                        else:
                            st.error(f"åŠ è½½å¿«ç…§ {snapshot['id'][-6:]} å¤±è´¥")
            with col3:
                if st.button("åˆ é™¤", key=f"del_{idx}_{snapshot['id']}", help=f"åˆ é™¤å¿«ç…§ {snapshot['id'][-6:]}"):
                    delete_success = delete_snapshot(snapshot['id'])
                    if delete_success:
                        st.success(f"å·²åˆ é™¤å¿«ç…§ {snapshot['id'][-6:]}")
                        st.rerun()
                    else:
                        st.error(f"åˆ é™¤å¿«ç…§ {snapshot['id'][-6:]} å¤±è´¥")

# æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ•°æ®æ–‡ä»¶", type=["xlsx", "xls"])

if uploaded_file is not None:
    st.session_state.uploaded_file_processed = True
    
    df = pd.read_excel(uploaded_file)
    st.session_state.original_df = df.copy()
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['Q', 'Passport_id', 'Value', 'product_st_new', 'brand']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {', '.join(missing_columns)}")
        st.stop()
    
    q_values = sorted(df['Q'].unique().tolist())
    
    # äº§å“-å“ç‰Œæ˜ å°„
    brand_mapping = df[['product_st_new', 'brand']].drop_duplicates().rename(
        columns={'product_st_new': 'äº§å“', 'brand': 'å“ç‰Œ'}
    )
    st.session_state.brand_mapping = brand_mapping
    
    # æ£€æŸ¥æœªåŒ¹é…å“ç‰Œçš„äº§å“
    missing_brand_products = brand_mapping[brand_mapping['å“ç‰Œ'].isna() | (brand_mapping['å“ç‰Œ'] == '')]
    if not missing_brand_products.empty:
        st.warning(f"å‘ç° {len(missing_brand_products)} ä¸ªäº§å“æœªåŒ¹é…åˆ°å“ç‰Œä¿¡æ¯")
    
    # å¯ç”¨äº§å“åˆ—è¡¨
    special_tags = ["æ–°å¢é—¨åº—", "é—¨åº—æµå¤±", "äº§å“æµå¤±", "æ–°å¢äº§å“", "å…¶ä»–äº§å“"]
    available_products = [p for p in brand_mapping['äº§å“'].unique() 
                         if p not in special_tags and pd.notna(p)]
    
    # å‚æ•°è®¾ç½®
    st.subheader("åˆ†æå‚æ•°è®¾ç½®")
    col1, col2, col3 = st.columns(3)
    with col1:
        start_period = st.selectbox("é€‰æ‹©æœŸåˆ", q_values, index=0)
        end_period = st.selectbox("é€‰æ‹©æœŸæœ«", q_values, index=min(1, len(q_values)-1))
        st.session_state.start_period = start_period
        st.session_state.end_period = end_period
        
        if start_period == end_period:
            st.error("æœŸåˆå’ŒæœŸæœ«ä¸èƒ½ç›¸åŒï¼Œè¯·é‡æ–°é€‰æ‹©")
            st.stop()
    
    with col2:
        if available_products:
            st.session_state.selected_product = st.selectbox(
                "é€‰æ‹©è¦åˆ†æçš„äº§å“", 
                available_products,
                index=0 if len(available_products) > 0 else None
            )
            
            product_brand = brand_mapping[
                brand_mapping['äº§å“'] == st.session_state.selected_product
            ]['å“ç‰Œ'].values[0] if st.session_state.selected_product in brand_mapping['äº§å“'].values else "æœªçŸ¥å“ç‰Œ"
            
            st.info(f"å¯¹åº”å“ç‰Œ: {product_brand}")
        else:
            st.warning("æœªæ‰¾åˆ°å¯ç”¨äº§å“æ•°æ®")
    
    with col3:
        use_full_products = st.checkbox("å…¨é‡è¾“å‡ºæ‰€æœ‰äº§å“", value=False)
        if not use_full_products:
            top_n = st.slider("å±•ç¤ºçš„Top Näº§å“æ•°é‡", 5, 20, 10)
        else:
            st.slider("å±•ç¤ºçš„Top Näº§å“æ•°é‡ï¼ˆå…¨é‡æ¨¡å¼ç¦ç”¨ï¼‰", 5, 20, 10, disabled=True)
        
        generate_data = st.button("ç”Ÿæˆæµé‡æ•°æ®å¹¶åˆ†æ")
    
    if generate_data and st.session_state.selected_product:
        with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
            # æ•°æ®å¤„ç†é€»è¾‘
            if 'Value U' not in df.columns:
                df = df.rename(columns={'Value': 'Value U'})
            df['product_processed'] = df['product_st_new']
            
            # è®¡ç®—æµå‘æ•°æ®
            flow_results = []
            for passport_id, group in df.groupby('Passport_id'):
                has_start = start_period in group['Q'].values
                has_end = end_period in group['Q'].values
                
                # æ–°å¢ç”¨æˆ·
                if not has_start and has_end:
                    product_data = group[group['Q'] == end_period].groupby('product_processed')['Value U'].sum().reset_index()
                    for _, row in product_data.iterrows():
                        flow_results.append({
                            'æœŸåˆäº§å“': "æ–°å¢é—¨åº—",
                            'æœŸæœ«äº§å“': row['product_processed'],
                            'æµé‡': row['Value U']
                        })
                    continue
                
                # æµå¤±ç”¨æˆ·
                if has_start and not has_end:
                    product_data = group[group['Q'] == start_period].groupby('product_processed')['Value U'].sum().reset_index()
                    for _, row in product_data.iterrows():
                        flow_results.append({
                            'æœŸåˆäº§å“': row['product_processed'],
                            'æœŸæœ«äº§å“': "é—¨åº—æµå¤±",
                            'æµé‡': row['Value U']
                        })
                    continue
                
                # æ—¢æœ‰æœŸåˆåˆæœ‰æœŸæœ«
                if has_start and has_end:
                    start_data = group[group['Q'] == start_period].copy()
                    end_data = group[group['Q'] == end_period].copy()
                    
                    start_agg = start_data.groupby('product_processed')['Value U'].sum().reset_index()
                    end_agg = end_data.groupby('product_processed')['Value U'].sum().reset_index()
                    
                    start_dict = {row['product_processed']: row['Value U'] for _, row in start_agg.iterrows()}
                    end_dict = {row['product_processed']: row['Value U'] for _, row in end_agg.iterrows()}
                    
                    # ç›¸åŒäº§å“ç•™å­˜
                    same_products = set(start_dict.keys()) & set(end_dict.keys())
                    for product in same_products:
                        flow = min(start_dict[product], end_dict[product])
                        flow_results.append({
                            'æœŸåˆäº§å“': product,
                            'æœŸæœ«äº§å“': product,
                            'æµé‡': flow
                        })
                        start_dict[product] -= flow
                        end_dict[product] -= flow
                        if start_dict[product] == 0: del start_dict[product]
                        if end_dict[product] == 0: del end_dict[product]
                    
                    # ä¸åŒäº§å“è½¬æ¢
                    remaining_start = list(start_dict.items())
                    remaining_end = list(end_dict.items())
                    
                    while remaining_start and remaining_end:
                        (s_product, s_val) = remaining_start[0]
                        (e_product, e_val) = remaining_end[0]
                        
                        flow = min(s_val, e_val)
                        flow_results.append({
                            'æœŸåˆäº§å“': s_product,
                            'æœŸæœ«äº§å“': e_product,
                            'æµé‡': flow
                        })
                        
                        if s_val == flow:
                            remaining_start.pop(0)
                        else:
                            remaining_start[0] = (s_product, s_val - flow)
                        
                        if e_val == flow:
                            remaining_end.pop(0)
                        else:
                            remaining_end[0] = (e_product, e_val - flow)
                    
                    # å‰©ä½™æµå¤±
                    for (s_product, s_val) in remaining_start:
                        flow_results.append({
                            'æœŸåˆäº§å“': s_product,
                            'æœŸæœ«äº§å“': "äº§å“æµå¤±",
                            'æµé‡': s_val
                        })
                    
                    # å‰©ä½™æ–°å¢
                    for (e_product, e_val) in remaining_end:
                        flow_results.append({
                            'æœŸåˆäº§å“': "æ–°å¢äº§å“",
                            'æœŸæœ«äº§å“': e_product,
                            'æµé‡': e_val
                        })
            
            # åˆå¹¶æµé‡æ•°æ®
            flow_df = pd.DataFrame(flow_results, columns=['æœŸåˆäº§å“', 'æœŸæœ«äº§å“', 'æµé‡'])
            flow_df = flow_df.groupby(['æœŸåˆäº§å“', 'æœŸæœ«äº§å“'], as_index=False)['æµé‡'].sum()
            st.session_state.flow_df = flow_df
            
            # æ‰“æ ‡åˆ†æ
            with st.spinner("æ­£åœ¨æ‰“æ ‡åˆ†æ..."):
                brand_dict = dict(zip(brand_mapping['äº§å“'], brand_mapping['å“ç‰Œ']))
                marked_df = flow_df.copy()
                
                marked_df['æœŸåˆå“ç‰Œ'] = marked_df['æœŸåˆäº§å“'].apply(
                    lambda x: brand_dict.get(x, x) if x not in special_tags else x
                )
                marked_df['æœŸæœ«å“ç‰Œ'] = marked_df['æœŸæœ«äº§å“'].apply(
                    lambda x: brand_dict.get(x, x) if x not in special_tags else x
                )
                
                product_brand = brand_mapping[
                    brand_mapping['äº§å“'] == st.session_state.selected_product
                ]['å“ç‰Œ'].values[0] if st.session_state.selected_product in brand_mapping['äº§å“'].values else "æœªçŸ¥å“ç‰Œ"
                
                # æ‰“æ ‡å‡½æ•°
                def create_labels(row):
                    if row['æœŸæœ«äº§å“'] == st.session_state.selected_product:
                        if row['æœŸåˆäº§å“'] in ["æ–°å¢é—¨åº—", "æ–°å¢äº§å“"]:
                            return "æœŸåˆ_æ–°å¢"
                        
                        start_product = row['æœŸåˆäº§å“']
                        start_brand = row['æœŸåˆå“ç‰Œ']
                        
                        if start_product == st.session_state.selected_product and start_brand == product_brand:
                            return "åŒå“ç‰ŒåŒäº§å“"
                        elif start_brand == product_brand:
                            return "æœŸåˆ_åŒå“ç‰Œä¸åŒäº§å“"
                        else:
                            return "æœŸåˆ_ä¸åŒå“ç‰Œä¸åŒäº§å“"
                    
                    elif row['æœŸåˆäº§å“'] == st.session_state.selected_product:
                        if row['æœŸæœ«äº§å“'] in ["é—¨åº—æµå¤±", "äº§å“æµå¤±"]:
                            return "æœŸæœ«_æµå¤±"
                        
                        end_product = row['æœŸæœ«äº§å“']
                        end_brand = row['æœŸæœ«å“ç‰Œ']
                        
                        if end_product == st.session_state.selected_product and end_brand == product_brand:
                            return "åŒå“ç‰ŒåŒäº§å“"
                        elif end_brand == product_brand:
                            return "æœŸæœ«_åŒå“ç‰Œä¸åŒäº§å“"
                        else:
                            return "æœŸæœ«_ä¸åŒå“ç‰Œä¸åŒäº§å“"
                    
                    else:
                        return "ä¸æ¶‰åŠç›®æ ‡äº§å“"
                
                marked_df['æ ‡ç­¾ç±»åˆ«'] = marked_df.apply(create_labels, axis=1)
                st.session_state.marked_data = marked_df.drop(['æœŸåˆå“ç‰Œ', 'æœŸæœ«å“ç‰Œ'], axis=1)
                
                # æ‹†åˆ†æµå‘ - å®Œæ•´æ•°æ®é›†
                split_flow = []
                for _, row in marked_df.iterrows():
                    if row['æ ‡ç­¾ç±»åˆ«'] != "ä¸æ¶‰åŠç›®æ ‡äº§å“":
                        split_flow.append({
                            'æºèŠ‚ç‚¹': row['æœŸåˆäº§å“'],
                            'ç›®æ ‡èŠ‚ç‚¹': row['æ ‡ç­¾ç±»åˆ«'],
                            'æµé‡': row['æµé‡'],
                            'æµå‘ç±»å‹': 'æœŸåˆåˆ°æ ‡ç­¾',
                            'æ ‡ç­¾ç±»åˆ«': row['æ ‡ç­¾ç±»åˆ«']
                        })
                        split_flow.append({
                            'æºèŠ‚ç‚¹': row['æ ‡ç­¾ç±»åˆ«'],
                            'ç›®æ ‡èŠ‚ç‚¹': row['æœŸæœ«äº§å“'],
                            'æµé‡': row['æµé‡'],
                            'æµå‘ç±»å‹': 'æ ‡ç­¾åˆ°æœŸæœ«',
                            'æ ‡ç­¾ç±»åˆ«': row['æ ‡ç­¾ç±»åˆ«']
                        })
                
                split_flow_df = pd.DataFrame(split_flow)
                if not split_flow_df.empty:
                    split_flow_df = split_flow_df.groupby(
                        ['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹', 'æµå‘ç±»å‹', 'æ ‡ç­¾ç±»åˆ«'], 
                        as_index=False
                    )['æµé‡'].sum()
                    
                    # æ’åº
                    split_flow_df['sort_key'] = split_flow_df['æ ‡ç­¾ç±»åˆ«'].apply(
                        lambda x: label_sort_mapping.get(x, len(LABEL_ORDER))
                    )
                    sorted_split_df = split_flow_df.sort_values(
                        by=['sort_key', 'æµé‡'], 
                        ascending=[True, False]
                    ).drop(columns=['sort_key'])
                    
                    st.session_state.split_flow_data = split_flow_df
                    st.session_state.sorted_split_data = sorted_split_df
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°æ¶‰åŠç›®æ ‡äº§å“çš„æµé‡æ•°æ®")
            
            # å¤„ç†æœŸåˆäº§å“ç­›é€‰æ•°æ®
            with st.spinner("æ­£åœ¨å¤„ç†æœŸåˆäº§å“ç­›é€‰æ•°æ®..."):
                if st.session_state.marked_data is not None:
                    start_filtered_data = st.session_state.marked_data[
                        st.session_state.marked_data['æœŸåˆäº§å“'] == st.session_state.selected_product
                    ]
                    
                    if not start_filtered_data.empty:
                        split_flow_start = []
                        for _, row in start_filtered_data.iterrows():
                            if row['æ ‡ç­¾ç±»åˆ«'] != "ä¸æ¶‰åŠç›®æ ‡äº§å“":
                                split_flow_start.append({
                                    'æºèŠ‚ç‚¹': row['æœŸåˆäº§å“'],
                                    'ç›®æ ‡èŠ‚ç‚¹': row['æ ‡ç­¾ç±»åˆ«'],
                                    'æµé‡': row['æµé‡'],
                                    'æµå‘ç±»å‹': 'æœŸåˆåˆ°æ ‡ç­¾',
                                    'æ ‡ç­¾ç±»åˆ«': row['æ ‡ç­¾ç±»åˆ«']
                                })
                                split_flow_start.append({
                                    'æºèŠ‚ç‚¹': row['æ ‡ç­¾ç±»åˆ«'],
                                    'ç›®æ ‡èŠ‚ç‚¹': row['æœŸæœ«äº§å“'],
                                    'æµé‡': row['æµé‡'],
                                    'æµå‘ç±»å‹': 'æ ‡ç­¾åˆ°æœŸæœ«',
                                    'æ ‡ç­¾ç±»åˆ«': row['æ ‡ç­¾ç±»åˆ«']
                                })
                        
                        split_flow_start_df = pd.DataFrame(split_flow_start)
                        if not split_flow_start_df.empty:
                            split_flow_start_df = split_flow_start_df.groupby(
                                ['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹', 'æµå‘ç±»å‹', 'æ ‡ç­¾ç±»åˆ«'], 
                                as_index=False
                            )['æµé‡'].sum()
                            
                            split_flow_start_df['sort_key'] = split_flow_start_df['æ ‡ç­¾ç±»åˆ«'].apply(
                                lambda x: label_sort_mapping.get(x, len(LABEL_ORDER))
                            )
                            sorted_split_start_df = split_flow_start_df.sort_values(
                                by=['sort_key', 'æµé‡'], 
                                ascending=[True, False]
                            ).drop(columns=['sort_key'])
                            
                            st.session_state.split_flow_data_start = split_flow_start_df
                            st.session_state.sorted_split_data_start = sorted_split_start_df
            
            # å¤„ç†æœŸæœ«äº§å“ç­›é€‰æ•°æ®
            with st.spinner("æ­£åœ¨å¤„ç†æœŸæœ«äº§å“ç­›é€‰æ•°æ®..."):
                if st.session_state.marked_data is not None:
                    end_filtered_data = st.session_state.marked_data[
                        st.session_state.marked_data['æœŸæœ«äº§å“'] == st.session_state.selected_product
                    ]
                    
                    if not end_filtered_data.empty:
                        split_flow_end = []
                        for _, row in end_filtered_data.iterrows():
                            if row['æ ‡ç­¾ç±»åˆ«'] != "ä¸æ¶‰åŠç›®æ ‡äº§å“":
                                split_flow_end.append({
                                    'æºèŠ‚ç‚¹': row['æœŸåˆäº§å“'],
                                    'ç›®æ ‡èŠ‚ç‚¹': row['æ ‡ç­¾ç±»åˆ«'],
                                    'æµé‡': row['æµé‡'],
                                    'æµå‘ç±»å‹': 'æœŸåˆåˆ°æ ‡ç­¾',
                                    'æ ‡ç­¾ç±»åˆ«': row['æ ‡ç­¾ç±»åˆ«']
                                })
                                split_flow_end.append({
                                    'æºèŠ‚ç‚¹': row['æ ‡ç­¾ç±»åˆ«'],
                                    'ç›®æ ‡èŠ‚ç‚¹': row['æœŸæœ«äº§å“'],
                                    'æµé‡': row['æµé‡'],
                                    'æµå‘ç±»å‹': 'æ ‡ç­¾åˆ°æœŸæœ«',
                                    'æ ‡ç­¾ç±»åˆ«': row['æ ‡ç­¾ç±»åˆ«']
                                })
                        
                        split_flow_end_df = pd.DataFrame(split_flow_end)
                        if not split_flow_end_df.empty:
                            split_flow_end_df = split_flow_end_df.groupby(
                                ['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹', 'æµå‘ç±»å‹', 'æ ‡ç­¾ç±»åˆ«'], 
                                as_index=False
                            )['æµé‡'].sum()
                            
                            split_flow_end_df['sort_key'] = split_flow_end_df['æ ‡ç­¾ç±»åˆ«'].apply(
                                lambda x: label_sort_mapping.get(x, len(LABEL_ORDER))
                            )
                            sorted_split_end_df = split_flow_end_df.sort_values(
                                by=['sort_key', 'æµé‡'], 
                                ascending=[True, False]
                            ).drop(columns=['sort_key'])
                            
                            st.session_state.split_flow_data_end = split_flow_end_df
                            st.session_state.sorted_split_data_end = sorted_split_end_df
            
            # ç”Ÿæˆæ¡‘åŸºå›¾
            with st.spinner("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨..."):
                # è®¡ç®—TopNäº§å“
                all_products = pd.unique(st.session_state.sorted_split_data[['æºèŠ‚ç‚¹', 'ç›®æ ‡èŠ‚ç‚¹']].values.ravel('K')) if st.session_state.sorted_split_data is not None else []
                product_traffic = {}
                
                for product in all_products:
                    if product not in special_tags and product not in MIDDLE_LAYER_ORDER:
                        source_flow = st.session_state.sorted_split_data[
                            st.session_state.sorted_split_data['æºèŠ‚ç‚¹'] == product
                        ]['æµé‡'].sum() if st.session_state.sorted_split_data is not None else 0
                        target_flow = st.session_state.sorted_split_data[
                            st.session_state.sorted_split_data['ç›®æ ‡èŠ‚ç‚¹'] == product
                        ]['æµé‡'].sum() if st.session_state.sorted_split_data is not None else 0
                        product_traffic[product] = source_flow + target_flow
                
                # é€‰æ‹©TopNäº§å“
                if product_traffic and not use_full_products:
                    sorted_products = sorted(product_traffic.items(), key=lambda x: x[1], reverse=True)
                    st.session_state.top_products = [p[0] for p in sorted_products[:top_n]]
                    st.info(f"å·²é€‰æ‹©æµé‡æœ€é«˜çš„Top {top_n} äº§å“")
                else:
                    st.session_state.top_products = None
                
                # ç”Ÿæˆå®Œæ•´å›¾è¡¨
                if st.session_state.sorted_split_data is not None and not st.session_state.sorted_split_data.empty:
                    st.session_state.sankey_fig = generate_sorted_sankey(
                        st.session_state.sorted_split_data,
                        top_products=st.session_state.top_products,
                        use_full_products=use_full_products
                    )
                
                # ç”ŸæˆæœŸåˆäº§å“ç­›é€‰å›¾è¡¨
                if st.session_state.sorted_split_data_start is not None and not st.session_state.sorted_split_data_start.empty:
                    st.session_state.sankey_fig_start = generate_sorted_sankey(
                        st.session_state.sorted_split_data_start,
                        top_products=st.session_state.top_products,
                        use_full_products=use_full_products
                    )
                
                # ç”ŸæˆæœŸæœ«äº§å“ç­›é€‰å›¾è¡¨
                if st.session_state.sorted_split_data_end is not None and not st.session_state.sorted_split_data_end.empty:
                    st.session_state.sankey_fig_end = generate_sorted_sankey(
                        st.session_state.sorted_split_data_end,
                        top_products=st.session_state.top_products,
                        use_full_products=use_full_products
                    )
            
            # ä¿å­˜å¿«ç…§
            save_success = save_snapshot(start_period, end_period, st.session_state.selected_product)
            if save_success:
                st.success("åˆ†æå®Œæˆå¹¶å·²ä¿å­˜å¿«ç…§")
            else:
                st.success("åˆ†æå®Œæˆ")
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if st.session_state.sorted_split_data is not None and not st.session_state.sorted_split_data.empty:
        # æ˜¾ç¤ºæ¡‘åŸºå›¾
        st.subheader("ğŸ“Š æµé‡å¯è§†åŒ–å›¾è¡¨")
        
        # å®Œæ•´æ¡‘åŸºå›¾
        st.subheader("å®Œæ•´äº§å“æµé‡æ¡‘åŸºå›¾")
        if st.session_state.sankey_fig is not None:
            st.plotly_chart(st.session_state.sankey_fig, use_container_width=True, config={'displayModeBar': True})
        else:
            st.warning("æ— æ³•ç”Ÿæˆå®Œæ•´æ¡‘åŸºå›¾")
        
        # æœŸåˆäº§å“ç­›é€‰æ¡‘åŸºå›¾
        if st.session_state.sorted_split_data_start is not None and not st.session_state.sorted_split_data_start.empty:
            st.subheader(f"æœŸåˆäº§å“ = {st.session_state.selected_product} çš„æµé‡æ¡‘åŸºå›¾")
            if st.session_state.sankey_fig_start is not None:
                st.plotly_chart(st.session_state.sankey_fig_start, use_container_width=True, config={'displayModeBar': True})
        
        # æœŸæœ«äº§å“ç­›é€‰æ¡‘åŸºå›¾
        if st.session_state.sorted_split_data_end is not None and not st.session_state.sorted_split_data_end.empty:
            st.subheader(f"æœŸæœ«äº§å“ = {st.session_state.selected_product} çš„æµé‡æ¡‘åŸºå›¾")
            if st.session_state.sankey_fig_end is not None:
                st.plotly_chart(st.session_state.sankey_fig_end, use_container_width=True, config={'displayModeBar': True})
        
        # æ˜¾ç¤ºæŠ¥å‘Š
        st.subheader("ğŸ“‹ åˆ†ææŠ¥å‘Š")
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæœŸæœ«åˆ†ææŠ¥å‘Š
        end_report = generate_end_report(st.session_state.marked_data, st.session_state.selected_product)
        st.markdown(end_report)
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæœŸåˆåˆ†ææŠ¥å‘Š
        start_report = generate_start_report(st.session_state.marked_data, st.session_state.selected_product)
        st.markdown(start_report)
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæ ‡ç­¾æ±‡æ€»æŠ¥å‘Š
        label_report = generate_label_summary(st.session_state.marked_data)
        st.markdown(label_report)
        
        # ä¸‹è½½ç»“æœ
        st.subheader("ğŸ’¾ ä¸‹è½½åˆ†æç»“æœ")
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            if st.session_state.marked_data is not None:
                st.session_state.marked_data.to_excel(writer, index=False, sheet_name='åŸå§‹æ‰“æ ‡æ•°æ®')
            
            st.session_state.sorted_split_data.drop(columns=['æ ‡ç­¾ç±»åˆ«']).to_excel(
                writer, index=False, sheet_name='å®Œæ•´æ‹†åˆ†æµå‘æ•°æ®'
            )
            
            if st.session_state.sorted_split_data_start is not None and not st.session_state.sorted_split_data_start.empty:
                st.session_state.sorted_split_data_start.drop(columns=['æ ‡ç­¾ç±»åˆ«']).to_excel(
                    writer, index=False, sheet_name='æœŸåˆäº§å“ç­›é€‰æ•°æ®'
                )
            
            if st.session_state.sorted_split_data_end is not None and not st.session_state.sorted_split_data_end.empty:
                st.session_state.sorted_split_data_end.drop(columns=['æ ‡ç­¾ç±»åˆ«']).to_excel(
                    writer, index=False, sheet_name='æœŸæœ«äº§å“ç­›é€‰æ•°æ®'
                )
            
            if st.session_state.top_products:
                pd.DataFrame({'TopNäº§å“': st.session_state.top_products}).to_excel(
                    writer, index=False, sheet_name='TopNäº§å“åˆ—è¡¨'
                )
        
        output.seek(0)
        if st.session_state.selected_product and st.session_state.start_period and st.session_state.end_period:
            st.download_button(
                "ä¸‹è½½å®Œæ•´ç»“æœï¼ˆExcelï¼‰",
                data=output,
                file_name=f"{st.session_state.start_period}_to_{st.session_state.end_period}_{st.session_state.selected_product}_ç»“æœ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        
        # å½“å‰å¿«ç…§ä¿¡æ¯
        if st.session_state.current_snapshot_id:
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"å½“å‰åˆ†æå·²ä¿å­˜ä¸ºå¿«ç…§ (ID: {st.session_state.current_snapshot_id[-6:]})")
            with col2:
                if st.button("é‡æ–°ä¿å­˜å¿«ç…§") and st.session_state.start_period and st.session_state.selected_product:
                    save_success = save_snapshot(st.session_state.start_period, st.session_state.end_period, st.session_state.selected_product)
                    if save_success:
                        st.success("å·²é‡æ–°ä¿å­˜å¿«ç…§")
elif uploaded_file is None:
    # å¦‚æœæœ‰åŠ è½½çš„å¿«ç…§ï¼Œæ˜¾ç¤ºç»“æœ
    if st.session_state.sorted_split_data is not None and not st.session_state.sorted_split_data.empty:
        st.info("æ˜¾ç¤ºå·²åŠ è½½çš„å¿«ç…§æ•°æ®")
        
        # æ˜¾ç¤ºæ¡‘åŸºå›¾
        st.subheader("ğŸ“Š æµé‡å¯è§†åŒ–å›¾è¡¨")
        
        # å®Œæ•´æ¡‘åŸºå›¾
        st.subheader("å®Œæ•´äº§å“æµé‡æ¡‘åŸºå›¾")
        if st.session_state.sankey_fig is not None:
            st.plotly_chart(st.session_state.sankey_fig, use_container_width=True, config={'displayModeBar': True})
        
        # æœŸåˆäº§å“ç­›é€‰æ¡‘åŸºå›¾
        if st.session_state.sorted_split_data_start is not None and not st.session_state.sorted_split_data_start.empty:
            st.subheader(f"æœŸåˆäº§å“ = {st.session_state.selected_product} çš„æµé‡æ¡‘åŸºå›¾")
            if st.session_state.sankey_fig_start is not None:
                st.plotly_chart(st.session_state.sankey_fig_start, use_container_width=True, config={'displayModeBar': True})
        
        # æœŸæœ«äº§å“ç­›é€‰æ¡‘åŸºå›¾
        if st.session_state.sorted_split_data_end is not None and not st.session_state.sorted_split_data_end.empty:
            st.subheader(f"æœŸæœ«äº§å“ = {st.session_state.selected_product} çš„æµé‡æ¡‘åŸºå›¾")
            if st.session_state.sankey_fig_end is not None:
                st.plotly_chart(st.session_state.sankey_fig_end, use_container_width=True, config={'displayModeBar': True})
        
        # æ˜¾ç¤ºæŠ¥å‘Š
        st.subheader("ğŸ“‹ åˆ†ææŠ¥å‘Š")
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæœŸæœ«åˆ†ææŠ¥å‘Š
        end_report = generate_end_report(st.session_state.marked_data, st.session_state.selected_product)
        st.markdown(end_report)
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæœŸåˆåˆ†ææŠ¥å‘Š
        start_report = generate_start_report(st.session_state.marked_data, st.session_state.selected_product)
        st.markdown(start_report)
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæ ‡ç­¾æ±‡æ€»æŠ¥å‘Š
        label_report = generate_label_summary(st.session_state.marked_data)
        st.markdown(label_report)
    
    else:
        st.info("è¯·ä¸Šä¼ åŒ…å«ä»¥ä¸‹åˆ—çš„æ•°æ®æ–‡ä»¶ï¼šPassport_id, Value, Q, product_st_new, brand")
